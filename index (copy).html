<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<title> EgoHandNet </title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#a90342">
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/normalize.css" media="screen">
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/stylesheet.css" media="screen">
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/github-light.css" media="screen">
	<link rel="shortcut icon" href="handgestar_img/favicon.ico" />
	<script type="text/javascript" async
	  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<!-- <script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-101653083-3', 'auto');
		ga('send', 'pageview');
	</script> -->
</head>

<body>
	<section class="page-header">
		<h1 class="project-name"> EgoHandNet </h1>
		<h2 class="project-tagline">EgoHandNet: A lightweight network for First Person View hand detection in Wild</h2>
<!-- 		<h2 class="project-authors"><a class = ".a_light" href="https://www.linkedin.com/in/neel-rakholia/"><b>Neel Rakholia\(^*\)</b></a> ,<a href="http://home.iiitd.edu.in/~srinidhi13164/"><b>Srinidhi Hegde\(^*\)</b></a>, <a href="https://scholar.google.co.in/citations?user=IJjnjZIAAAAJ&hl=en"><b>Ramya Hebbalaguppe</b></a></h2> -->
		<a href="#video1" class="btn">Demo Video</a>
		<a href="#app1" class="btn">EgoGestAR Dataset</a>
		<a href="#poster" class="btn">Poster</a>
		<!-- <a href="#video2" class="btn">Demo Video 2</a>
		<a href="https://github.com/handgestar/EgoGestAR" target="_blank" class="btn">EgoGestAR Dataset and Codebase</a>
		 <a href="https://github.com/handgestar/HandGestAR" target="_blank" class="btn">Videos Dataset for Testing</a>-->
	</section>

	<section class="main-content">
		<h3><a id="welcome-to-handgestar" class="anchor" href="#welcome-to-handgestar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h3>
		<p align="justify"> Hand gestures are utilised extensively on egocentric vision systems, in robotics, Augmented and Virtual Reality systems, amongst others. Accurate hand detection is typically a first step for the applications mentioned before and can be achieved through the state-of-the-art deep learning models such as YOLOv2, MobileNetv2 and Faster RCNN  Often, such models come with overhead of either computational load or failure to achieve real-time performance in a resource constrained environment. To this end, we propose a light weight hand detection module that also achieves state-of-the-art performance in First Person View(FPV) using RGB data only. Our network, termed, EgoHandNet is a hand detection module inspired by Tiny YOLO[] preserving the global context for accurate localisation of hands in FPV. Our contributions to hand detection are: low model size (73MB) can be easily ported on phone with highest precision among the existing models such as TinyYOLO, YOLOv2, FRCNN and also MobileNetV2 99.88% on SCUT dataset. This is mainly because our novel loss function used for training the network and for specific purpose of detecting hand in FPV. Our
model is trained on a GPU machine and is ported on to an android device to be used with frugal Augmented Reality headsets in the likes of Google Cardboard and VR Box. </p>

		<p><img src="handgestar_img/showimg.png"></p>

		<h3><a id="welcome-to-handgestar" class="anchor" href="#welcome-to-handgestar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Key Contribution</h3>
		<ol>
			<li> We propose <i> EgoHandNet </i>, a light weight convolutional neural network architecture, consisting of a CNN for efficient hand Detection in First person view gestural interaction with HMD.
			<li> <i> EgoHandNet </i> works in real-time and can be ported on mobile devices due to low memory footprint.
			<li> <b> Loss function </b>: To fit the box perfectly.
		</ol>

		<h3><a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

		<p><img src="handgestar_img/drawinair_pipeline.png"></p><!-- https://handgestar.github.io/ -->
		
		<p align="justify"> In this work, we present a neural network architecture comprising of a base CNN and a <em> differentiable spatial to numerical transform</em> (DSNT) layer followed by a Bidirectional Long Short-Term Memory(Bi-LSTM). The DSNT layer transforms the heatmap from CNN, that is rich in spatial information, to output spatial location of fingertip. The details of the CNN+DSNT network is mentioned in the figure below. The Bi-LSTM effectively captures the dynamic motion of user gesture that aids in classification.
			<!-- $$\begin{equation}
		<p><img src="handgestar_img/pipeline.png"></p> https://handgestar.github.io/ -->
		
		<p><img src="handgestar_img/ProposedMethod.png"></p>

		<p>Figure above shows the overview of our proposed <em>fingertip regressor</em> architecture for fingertip localization. The input to the network is  3x256x256 sized RGB images. The network consists of 6 convolutional blocks, each with different convolutional layers followed by a max-pooling layer. Then we have a convolutional layer to output a heatmap which is input to DSNT. Finally, we get 2 coordinates denoting fingertip spatial location.</p>

		<h3><a id="app1" class="anchor" href="#app1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>EgoGestAR Dataset</h3>

		<p><img src="handgestar_img/dataset.png"></p>

		<p>We collected the data from 50 subjects in our research lab with ages in the range 21 to 50 with average age 27.8 years. The dataset consists of 2500 gesture patterns where each subject recorded 5 samples of each gesture. The gestures were recorded by mounting a 10.1 inch display HP Pro Tablet to a wall. The gesture pattern drawn by a user's index finger on a touch interface application with position sensing region was stored. The data was captured at a resolution of 640 x 480. Figure above describes the standard input sequences shown to the users before data collection and a sample subset of gestures from the dataset showing the variability introduced by the subjects. Statistics of the EgoGestAR dataset is shown below. The dataset is available <a href="https://github.com/varunj/EgoGestAR">here</a>.</p>

		<p><img src="handgestar_img/table_data.png"></p>

		<p align="justify"></p>


		<!-- Video for project -->
		<h3><a id="video1" class="anchor" href="#video1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video</h3>
		<div class="video-responsive">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/nGYor0bGXdc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
		</div>

		<h3><a id="poster" class="anchor" href="#poster" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Poster</h3>

		<p><img src="handgestar_img/poster.png"></p>


	</section>

</body>
</html>
