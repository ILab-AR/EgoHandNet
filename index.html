<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<title> EgoHandNet </title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#a90342">
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/normalize.css" media="screen">
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/stylesheet.css" media="screen">
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/github-light.css" media="screen">
	<link rel="shortcut icon" href="handgestar_img/favicon.ico" />
	<script type="text/javascript" async
	  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<!-- <script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-101653083-3', 'auto');
		ga('send', 'pageview');
	</script> -->
</head>

<body>
	<section class="page-header">
		<h1 class="project-name"> <i> EgoHandNet <i> </h1>
		<h2 class="project-tagline"> <i> EgoHandNet <i>: A lightweight network for first person view hand detection in Wild</h2>
<!-- 		<h2 class="project-authors"><a class = ".a_light" href="https://www.linkedin.com/in/neel-rakholia/"><b>Neel Rakholia\(^*\)</b></a> ,<a href="http://home.iiitd.edu.in/~srinidhi13164/"><b>Srinidhi Hegde\(^*\)</b></a>, <a href="https://scholar.google.co.in/citations?user=IJjnjZIAAAAJ&hl=en"><b>Ramya Hebbalaguppe</b></a></h2> -->
		<a href="#video1" class="btn">Demo Video</a>
		<!-- <a href="#video2" class="btn">Demo Video 2</a>
		<a href="https://github.com/handgestar/EgoGestAR" target="_blank" class="btn">EgoGestAR Dataset and Codebase</a>
		 <a href="https://github.com/handgestar/HandGestAR" target="_blank" class="btn">Videos Dataset for Testing</a>-->
	</section>

	<section class="main-content">
		<h3><a id="welcome-to-handgestar" class="anchor" href="#welcome-to-handgestar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h3>
		<p align="justify"> With pervasive wearable technology, hand gestures form an important mode of interaction. Hand detection is usually
		the first step for hand gestural interactions. Applications for hand detection are wide, in the fields of robotics, Augmented and
		Virtual Reality(AR/VR), Grab-and-go grocery stores amongst others. Hand detection can be achieved through the state-of-the-
		art deep learning models such as YOLOv2, MobileNetv2 and Faster RCNN. Often, such models are computationally
		expensive or fail to achieve real-time performance in a resource constrained environments such as frugal video see-through Head
		Mounted Devices(HMD). To this end, we propose a lightweight hand detection architecture that can be generically utilized on
		these HMDs. Recent work in the area has shown impressive progress using RGBD input. However, since the introduction of
		RGBD sensors, there has been little progress in algorithms that work solely on monocular color input. We capitalize on the latest
		advancements of deep learning to overcome the need of additional depth or IR sensors for accurate hand detection. Our lightweight
		hand detection architecture for egocentric systems is termed EgoHandNet. We demonstrate that EgoHandNet outperforms
		state-of-art in FPV using RGB images at both in terms of mean Average precision(mAP) of 99.88% with a very high frame rate of
		364 FPS. As hand detetection is momentous for many application. when other modules are appended to it, whole system will perform in real time.  EgoHandNet is a hand detection module inspired by YOLO to localize and classify hand in one go with an improved
		loss function that fits bounding box perfectly.
			
		<h3><a id="welcome-to-handgestar" align="justify" class="anchor" href="#welcome-to-handgestar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Key Contribution</h3>
		<ol>
			<li> We propose <i> EgoHandNet </i>, a light weight convolutional neural network architecture, consisting of a CNN for efficient hand Detection in First person view gestural interaction with HMD.
			<li> <i> EgoHandNet </i>  works in real-time and can be ported on mobile devices due to low memory footprint.
			<li> <b> Loss function </b>: A novel loss function is defined to fit the bounding box by penalizing by enablling reflection of small deviations in large boxes
			matter less than small deviations in small boxes. To address this we penalize equally for equal propportion of deviation of height and width of bounding box.
		</ol>

		<h3><a id="Proposed Architecture" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

		
		<p align="justify"> We present a novel light weight architecture to detect hands in first person view. Figure shows our architecture as a convolutional neural network with 9 Convolutional layers followed by two Fully Connected(FC1, FC2) layers. A dataset used for training is a combination of SCUT, KITTY and MS COCO. The training of the model is de-
scribed in subsection 3.1 and evaluated it on the combined dataset (SCUT+KITTY+COCO). The initial convolutional layers of the network extract features from the image while the fully connected layer
predicts the hand’s confidence score, with normalised hand centroid and normalized bounding box dimensions(height and width). Our network architecture is inspired by the YOLO and Tiny YOLO models for hand classification and detection. Our network has 9 convolutional layers, 4 pooling layer(after first, second ,sixth and seventh covolutional layer) followed by two fully connected layer. Batch
normalization leads to significant improvements in convergence while eliminating the need for other forms of regularization. So batch normalization is used after each convolution layer. Input image is 160 * 120. Size of input layer is kept small because we want to detect hand in FPV as if hand is present it will be most prominent object and it will also reduce computational complexity hence model size could be small. we use leaky ReLu activation function in the convolution layer, no activation in FC1 and sigmoid activationfunction in FC2 layer. So that we can get normalized out-
put ranges from 0 to 1. The final output of our network is a 1D vector of length 5.
			<!-- $$\begin{equation}
		<p><img src="handgestar_img/pipeline.png"></p> https://handgestar.github.io/ -->
		
		<p><img src="handgestar_img/proposedhand.png"></p>

		<p>Above figure shows the Architecture of our proposed network. </p>

		<h3><a id="app1" class="anchor" href="#app1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset</h3>

		

		<p align="justify"> We have chosen dataset, that is union of SCUT dataset, The KITTI Vision dataset and MS COCO datasets. SCUT dataset is used to train positive samples as it is the only benchmark dataset available for detection of hands in FPV, while KITTI Vision and MS COCO datasets are used for getting negative samples. All images were resized to 160 × 120 thereby reducing model size.
		We have to detect a bounding box containing hand in a frame which occupies a significant portion of image as it is captured in FPV. Randomly 55,551 images is selected from SCUT dataset and 17,033 from COCO and 22,000 from The KITTI Vision dataset. Out of these images, ramdomly 55,000 are selected for training, 15,000 for validation and 24,584 for testing 
		Input to our network is normalized image obtained by dividing each channel in RGB data by 255. For tuning the network, we trained for centroid localization using only 18,000 images from SCUT datasets for 16 epochs. Then using whole training set we train for hand classification. We trained for detection of centroid for 12 epochs.
		Finally, then we train with loss function which comprises of class probability, centroid coordinates, height and width of bounding box for 48 epochs.</p>


		<p align="justify"></p>
		<h3><a id="app1" class="anchor" href="#app1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h3>

		<p><img src="handgestar_img/results.jpg"></p>
		
		
		<!-- Video for project -->
		<h3><a id="video1" class="anchor" href="#video1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video</h3>
		<div class="video-responsive">
 <video width="640" height="480" controls>
  <source src="handgestar_img/EgoHandnet.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
		</div>


	</section>

</body>
</html>
