<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<title> EgoHandNet </title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#a90342">
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/normalize.css" media="screen">
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/stylesheet.css" media="screen">
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/github-light.css" media="screen">
	<link rel="shortcut icon" href="handgestar_img/favicon.ico" />
	<script type="text/javascript" async
	  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<!-- <script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-101653083-3', 'auto');
		ga('send', 'pageview');
	</script> -->
</head>

<body>
	<section class="page-header">
		<h1 class="project-name"> EgoHandNet </h1>
		<h2 class="project-tagline">EgoHandNet: A lightweight network for First Person View hand detection in Wild</h2>
<!-- 		<h2 class="project-authors"><a class = ".a_light" href="https://www.linkedin.com/in/neel-rakholia/"><b>Neel Rakholia\(^*\)</b></a> ,<a href="http://home.iiitd.edu.in/~srinidhi13164/"><b>Srinidhi Hegde\(^*\)</b></a>, <a href="https://scholar.google.co.in/citations?user=IJjnjZIAAAAJ&hl=en"><b>Ramya Hebbalaguppe</b></a></h2> -->
		<a href="#video1" class="btn">Demo Video</a>
		<!-- <a href="#video2" class="btn">Demo Video 2</a>
		<a href="https://github.com/handgestar/EgoGestAR" target="_blank" class="btn">EgoGestAR Dataset and Codebase</a>
		 <a href="https://github.com/handgestar/HandGestAR" target="_blank" class="btn">Videos Dataset for Testing</a>-->
	</section>

	<section class="main-content">
		<h3><a id="welcome-to-handgestar" class="anchor" href="#welcome-to-handgestar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h3>
		<p align="justify"> Hand gestures are utilised extensively on egocentric vision systems, in robotics, Augmented and Virtual Reality systems, amongst others. Accurate hand detection is typically a first step for the applications mentioned before and can be achieved through the state-of-the-art deep learning models such as YOLOv2, MobileNetv2 and Faster RCNN  Often, such models come with overhead of either computational load or failure to achieve real-time performance in a resource constrained environment. To this end, we propose a light weight hand detection module that also achieves state-of-the-art performance in First Person View(FPV) using RGB data only. Our network, termed, EgoHandNet is a hand detection module inspired by Tiny YOLO[] preserving the global context for accurate localisation of hands in FPV. Our contributions to hand detection are: low model size (73MB) can be easily ported on phone with highest precision among the existing models such as TinyYOLO, YOLOv2, FRCNN and also MobileNetV2 99.88% on SCUT dataset. This is mainly because our novel loss function used for training the network and for specific purpose of detecting hand in FPV. Our
model is trained on a GPU machine and is ported on to an android device to be used with frugal Augmented Reality headsets in the likes of Google Cardboard and VR Box. </p>


		<h3><a id="welcome-to-handgestar" class="anchor" href="#welcome-to-handgestar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Key Contribution</h3>
		<ol>
			<li> We propose <i> EgoHandNet </i>, a light weight convolutional neural network architecture, consisting of a CNN for efficient hand Detection in First person view gestural interaction with HMD.
			<li> <i> EgoHandNet </i>  works in real-time and can be ported on mobile devices due to low memory footprint.
			<li> <b> Loss function </b>: A novel loss function is defined to fit the bounding box by penalizing height and box evenly for all size boxes.
		</ol>

		<h3><a id="Proposed Architecture" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

		
		<p align="justify"> We present a novel light weight architecture to detect hands in first person view. Figure ?? shows our architecture as a convolutional neural network with 9 Convolutional layers followed by two Fully Connected(FC1, FC2) layers. A dataset used for training is a combination of SCUT, KITTY and MS COCO. The training of the model is de-
scribed in subsection 3.1 and evaluated it on the combined dataset (SCUT+KITTY+COCO). The initial convolutional layers of the network extract features from the image while the fully connected layer
predicts the handâ€™s confidence score, with normalised hand centroid and normalized bounding box dimensions(height and width). Our network architecture is inspired by the YOLO and Tiny YOLO models for hand classification and detection. Our network has 9 convolutional layers, 4 pooling layer(after first, second ,sixth and seventh covolutional layer) followed by two fully connected layer. Batch
normalization leads to significant improvements in convergence while eliminating the need for other forms of regularization. So batch normalization is used after each convolution layer. Input image is 160 * 120. Size of input layer is kept small because we want to detect hand in FPV as if hand is present it will be most prominent object and it will also reduce computational complexity hence model size could be small. we use leaky ReLu activation function in the convolution layer, no activation in FC1 and sigmoid activationfunction in FC2 layer. So that we can get normalized out-
put ranges from 0 to 1. The final output of our network is a 1D vector of length 5.
			<!-- $$\begin{equation}
		<p><img src="handgestar_img/pipeline.png"></p> https://handgestar.github.io/ -->
		
		<p><img src="handgestar_img/ProposedMethod.png"></p>

		<p>Figure above shows the overview of our proposed <em>fingertip regressor</em> architecture for fingertip localization. The input to the network is  3x256x256 sized RGB images. The network consists of 6 convolutional blocks, each with different convolutional layers followed by a max-pooling layer. Then we have a convolutional layer to output a heatmap which is input to DSNT. Finally, we get 2 coordinates denoting fingertip spatial location.</p>

		<h3><a id="app1" class="anchor" href="#app1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset</h3>

		

		<p>To train the net dataset is comprised of SCUT datasets, The KITTI Vision daatset and coco datasets. SCUT datasets is used for positive hand case, The KITTI Vision daatset and coco is used for negative hand case. All imges were resized to 160 X 120 sizes images. we have to detect only one object( Hand) which will be of sufficient size if present because image is capture in ego vision. which helps in reducing model size. Randomly 55551 images is selected from SCUT datasets and 39033 form coco and The KITTI Vision dataset . So totally 94584 images is used. out of these total
image, ramdomly 55000 is selected for training, 15000 for validation and 24584 for Testing</p>


		<p align="justify"></p>


		<!-- Video for project -->
		<h3><a id="video1" class="anchor" href="#video1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video</h3>
		<div class="video-responsive">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/nGYor0bGXdc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
		</div>


	</section>

</body>
</html>
